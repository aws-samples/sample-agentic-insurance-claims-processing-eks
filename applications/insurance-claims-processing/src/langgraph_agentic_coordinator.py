"""
LangGraph Claims Coordinator - Industry-Standard Claims Processing
Human-in-the-loop coordination with AI assistance and regulatory compliance
"""

import asyncio
import json
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional, TypedDict, Annotated
import operator

from fastapi import FastAPI, HTTPException, BackgroundTasks, WebSocket, WebSocketDisconnect
from pydantic import BaseModel

# LangGraph and LangChain imports
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from langchain_ollama import ChatOllama

# Import human workflow management
from .human_workflow_manager import HumanWorkflowManager, ClaimsRole, TaskPriority
from .database_models import db_manager
from .external_integrations.agentic_external_manager import agentic_external_manager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# State for human-supervised coordination
class CoordinatorState(TypedDict):
    messages: Annotated[List[BaseMessage], operator.add]
    claim_data: Dict[str, Any]
    agent_assignments: Dict[str, List[str]]
    agent_results: Dict[str, Any]
    coordination_strategy: str
    priority_level: int
    collaboration_needed: bool
    ai_recommendation: Optional[Dict[str, Any]]  # AI provides recommendations only
    human_routing_decision: Optional[Dict[str, Any]]  # Human makes final decisions
    reasoning_chain: List[str]
    regulatory_requirements: List[str]
    active_workflows: List[str]

@tool
def route_to_fraud_agent(claim_data: dict) -> dict:
    """Route claim to fraud detection agent based on risk indicators."""
    amount = claim_data.get("claim_amount", 0)
    description = claim_data.get("description", "")
    
    if amount > 25000 or "accident" in description.lower():
        return {
            "agent": "fraud_agent",
            "priority": "high",
            "reason": "High amount or accident-related claim"
        }
    return {
        "agent": "fraud_agent", 
        "priority": "normal",
        "reason": "Standard fraud screening"
    }

@tool
def route_to_policy_agent(claim_data: dict) -> dict:
    """Route claim to policy validation agent."""
    return {
        "agent": "policy_agent",
        "priority": "high",
        "reason": "Policy validation required for all claims"
    }

@tool
def determine_collaboration_strategy(agent_results: dict) -> dict:
    """Determine if agents need to collaborate based on initial results."""
    fraud_risk = agent_results.get("fraud_agent", {}).get("risk_level", "low")
    policy_issues = agent_results.get("policy_agent", {}).get("issues_found", [])
    
    collaboration_needed = False
    strategy = "parallel"
    
    if fraud_risk == "high" or len(policy_issues) > 0:
        collaboration_needed = True
        strategy = "sequential_investigation"
    
    return {
        "collaboration_needed": collaboration_needed,
        "strategy": strategy,
        "next_agents": ["investigation_agent"] if collaboration_needed else [],
        "reasoning": f"Fraud risk: {fraud_risk}, Policy issues: {len(policy_issues)}"
    }

@tool
def create_ai_recommendation(all_results: dict) -> dict:
    """Create AI recommendation for human review - NO AUTONOMOUS DECISIONS."""
    fraud_score = all_results.get("fraud_agent", {}).get("fraud_score", 0.0)
    policy_valid = all_results.get("policy_agent", {}).get("policy_valid", True)
    investigation_result = all_results.get("investigation_agent", {}).get("recommendation", "approve")

    # AI provides analysis and recommendation only - humans decide
    risk_factors = []
    regulatory_triggers = []

    if not policy_valid:
        risk_factors.append("Policy validation issues identified")
        regulatory_triggers.append("underwriter_review_required")

    if fraud_score > 0.7:
        risk_factors.append("High fraud probability detected")
        regulatory_triggers.append("siu_investigation_required")
    elif fraud_score > 0.4:
        risk_factors.append("Moderate fraud indicators present")
        regulatory_triggers.append("enhanced_investigation_required")

    if investigation_result == "investigate":
        risk_factors.append("Investigation agent recommends further review")
        regulatory_triggers.append("detailed_investigation_required")

    # Determine recommended human reviewer based on industry standards
    recommended_reviewer = "claims_adjuster"  # Default
    if fraud_score > 0.7:
        recommended_reviewer = "siu_investigator"
    elif not policy_valid:
        recommended_reviewer = "underwriter"
    elif all_results.get("claim_amount", 0) > 50000:
        recommended_reviewer = "senior_adjuster"

    return {
        "ai_recommendation_only": True,
        "fraud_score": fraud_score,
        "policy_valid": policy_valid,
        "risk_factors": risk_factors,
        "regulatory_triggers": regulatory_triggers,
        "recommended_reviewer": recommended_reviewer,
        "confidence_in_analysis": 0.85,
        "human_decision_required": True,
        "compliance_note": "AI analysis complete - Human decision required per industry standards"
    }

class LangGraphClaimsCoordinator:
    """
    Industry-standard claims coordinator with human oversight.

    Features:
    - AI-assisted analysis and recommendations
    - Human-in-the-loop decision making
    - Regulatory compliance enforcement
    - Licensed professional routing
    - Audit trail management
    """
    
    def __init__(self, ollama_endpoint: str = "http://ollama-service:11434"):
        self.coordinator_id = "langgraph_coordinator_001"
        self.ollama_endpoint = ollama_endpoint

        # Initialize LLM for analysis assistance (lower temperature for consistency)
        import os
        model_name = os.getenv("MODEL_NAME", "qwen3-coder")
        self.llm = ChatOllama(
            base_url=ollama_endpoint,
            model=model_name,
            temperature=0.3  # Lower for consistent analysis
        )

        # Initialize human workflow manager
        self.human_workflow_manager = HumanWorkflowManager()

        # Regulatory compliance tracking
        self.regulatory_requirements = {
            "first_notice_of_loss": "24_hours",
            "coverage_decision": "30_days",
            "fraud_reporting": "10_days",
            "claim_resolution": "90_days"
        }

        # Performance tracking for AI assistance quality
        self.ai_assistance_metrics = {
            "recommendations_provided": 0,
            "human_agreement_rate": 0.0,
            "processing_efficiency": 0.0
        }
        
        # Agent service endpoints
        self.agent_services = {
            "fraud_agent": "http://fraud-agent-service:8001",
            "policy_agent": "http://policy-agent-service:8002", 
            "investigation_agent": "http://investigation-agent-service:8003",
            "claim_expander": "http://claim-expander-service:8004"
        }
        
        # Create coordination workflow
        self.workflow = self._create_coordination_workflow()
        self.memory = MemorySaver()
        self.app = self.workflow.compile(checkpointer=self.memory)
        
        logger.info(f"Initialized Claims Coordinator with Human Oversight: {self.coordinator_id}")

    def _check_regulatory_requirements(self, claim_data: Dict[str, Any]) -> List[str]:
        """Check regulatory compliance requirements"""
        triggers = []

        claim_amount = claim_data.get("claim_amount", 0)
        jurisdiction = claim_data.get("jurisdiction", "unknown")

        # Large loss reporting requirements
        if claim_amount > 100000:
            triggers.append("large_loss_reporting_required")

        # Fraud reporting thresholds
        if claim_amount > 50000:
            triggers.append("fraud_screening_enhanced")

        # Time-sensitive requirements
        triggers.append("first_notice_24_hours")
        triggers.append("coverage_decision_30_days")

        # State-specific requirements
        if jurisdiction in ["CA", "NY", "FL"]:
            triggers.append("enhanced_consumer_protection")

        return triggers

    # Autonomous Learning and Adaptation Methods
    
    async def learn_from_outcome(self, claim_id: str, predicted_decision: str, actual_outcome: str):
        """Learn from prediction outcomes to improve future decisions."""
        learning_record = {
            "claim_id": claim_id,
            "predicted": predicted_decision,
            "actual": actual_outcome,
            "accuracy": 1.0 if predicted_decision == actual_outcome else 0.0,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        self.learning_memory[claim_id] = learning_record
        
        # Update performance metrics
        total_predictions = len(self.learning_memory)
        correct_predictions = sum(1 for record in self.learning_memory.values() if record["accuracy"] == 1.0)
        current_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 1.0
        
        # Adapt goals based on performance
        if current_accuracy < self.dynamic_goals["accuracy_target"]:
            self.dynamic_goals["primary_goal"] = "improve_accuracy"
            logger.info(f"Goal adapted: Focusing on accuracy improvement ({current_accuracy:.2f})")
        elif current_accuracy > 0.98:
            self.dynamic_goals["primary_goal"] = "optimize_speed"
            logger.info(f"Goal adapted: Focusing on speed optimization")
    
    async def generate_creative_solution(self, problem_context: Dict[str, Any]) -> Dict[str, Any]:
        """Generate novel approaches for complex cases that failed standard processing."""
        creative_prompt = f"""
        This insurance claim case has failed standard processing approaches:
        
        Problem: {problem_context}
        Previous Attempts: {problem_context.get('failed_methods', [])}
        
        Generate 3 completely novel investigation approaches that haven't been tried.
        Think outside standard fraud detection patterns. Be creative and innovative.
        
        Consider:
        - Unconventional data correlations
        - Cross-reference with external data sources
        - Social network analysis approaches
        - Behavioral pattern recognition
        - Time-series anomaly detection
        
        Provide specific, actionable approaches.
        """
        
        try:
            response = await self.llm.ainvoke([HumanMessage(content=creative_prompt)])
            creative_approaches = self._parse_creative_solutions(response.content)
            
            # Store novel solution for future use
            solution_id = f"creative_{len(self.creative_solutions)}"
            self.creative_solutions[solution_id] = {
                "approaches": creative_approaches,
                "problem_context": problem_context,
                "generated_at": datetime.utcnow().isoformat()
            }
            
            logger.info(f"ðŸŽ¨ CREATIVE SOLUTION GENERATED: {len(creative_approaches)} novel approaches")
            return {"solution_id": solution_id, "approaches": creative_approaches}
            
        except Exception as e:
            logger.error(f"Creative solution generation failed: {e}")
            return {"approaches": ["standard_investigation"]}
    
    def _parse_creative_solutions(self, llm_response: str) -> List[str]:
        """Parse LLM-generated creative solutions"""
        # Simple parsing - enhance with structured output
        approaches = []
        lines = llm_response.split('\n')
        for line in lines:
            if any(keyword in line.lower() for keyword in ['approach', 'method', 'technique', 'strategy']):
                approaches.append(line.strip())
        
        return approaches[:3] if approaches else ["novel_cross_correlation", "behavioral_analysis", "temporal_pattern_detection"]
    
    async def self_optimize_workflow(self):
        """WORKFLOW SELF-EVOLUTION: Dynamically optimize workflow based on performance"""
        # Analyze workflow performance patterns
        if len(self.learning_memory) < 10:
            return  # Need enough data points
        
        # Calculate performance metrics
        recent_records = list(self.learning_memory.values())[-10:]
        avg_accuracy = sum(r["accuracy"] for r in recent_records) / len(recent_records)
        
        # Optimize based on current goals
        optimization = None
        
        if self.dynamic_goals["primary_goal"] == "improve_accuracy" and avg_accuracy < 0.9:
            optimization = {
                "type": "add_validation_step",
                "description": "Add additional validation node for low-confidence decisions",
                "expected_improvement": "accuracy +15%"
            }
        elif self.dynamic_goals["primary_goal"] == "optimize_speed":
            optimization = {
                "type": "parallel_processing", 
                "description": "Enable more parallel agent execution",
                "expected_improvement": "speed +30%"
            }
        
        if optimization:
            self.workflow_optimizations.append({
                **optimization,
                "implemented_at": datetime.utcnow().isoformat()
            })
            logger.info(f"ðŸ”„ WORKFLOW OPTIMIZED: {optimization['description']}")
    
    async def autonomous_performance_assessment(self) -> Dict[str, Any]:
        """SELF-ASSESSMENT: Evaluate own performance and trigger improvements"""
        if not self.learning_memory:
            return {"status": "insufficient_data"}
        
        # Calculate performance metrics
        total_cases = len(self.learning_memory)
        accuracy = sum(1 for record in self.learning_memory.values() if record["accuracy"] == 1.0) / total_cases
        
        assessment = {
            "total_cases_processed": total_cases,
            "accuracy": accuracy,
            "current_goal": self.dynamic_goals["primary_goal"],
            "accuracy_vs_target": accuracy - self.dynamic_goals["accuracy_target"],
            "performance_trend": "improving" if accuracy > 0.85 else "needs_improvement",
            "recommended_actions": []
        }
        
        # Self-triggered improvements
        if accuracy < self.dynamic_goals["accuracy_target"]:
            assessment["recommended_actions"].append("initiate_learning_enhancement")
            await self.self_optimize_workflow()
        
        if accuracy > 0.98:
            assessment["recommended_actions"].append("focus_on_speed_optimization")
            self.dynamic_goals["primary_goal"] = "optimize_speed"
        
        logger.info(f"ðŸ“Š SELF-ASSESSMENT: Accuracy {accuracy:.2f}, Goal: {self.dynamic_goals['primary_goal']}")
        return assessment

    def _create_coordination_workflow(self) -> StateGraph:
        """Create the multi-agent coordination workflow"""
        
        workflow = StateGraph(CoordinatorState)
        
        # Workflow nodes
        workflow.add_node("analyze_claim", self._analyze_claim_node)
        workflow.add_node("route_agents", self._route_agents_node)
        workflow.add_node("execute_parallel", self._execute_parallel_node)
        workflow.add_node("evaluate_collaboration", self._evaluate_collaboration_node)
        workflow.add_node("coordinate_investigation", self._coordinate_investigation_node)
        workflow.add_node("aggregate_results", self._aggregate_results_node)
        workflow.add_node("human_routing", self._human_routing_node)
        
        # Workflow edges
        workflow.set_entry_point("analyze_claim")
        workflow.add_edge("analyze_claim", "route_agents")
        workflow.add_edge("route_agents", "execute_parallel")
        workflow.add_edge("execute_parallel", "evaluate_collaboration")
        
        # Conditional routing for collaboration
        workflow.add_conditional_edges(
            "evaluate_collaboration",
            self._needs_collaboration,
            {
                "collaborate": "coordinate_investigation",
                "aggregate": "aggregate_results"
            }
        )
        
        workflow.add_edge("coordinate_investigation", "aggregate_results")
        workflow.add_edge("aggregate_results", "human_routing")
        workflow.add_edge("human_routing", END)
        
        return workflow

    async def _analyze_claim_node(self, state: CoordinatorState) -> CoordinatorState:
        """ðŸ¤– Enhanced claim analysis with multi-source agentic intelligence"""
        claim_data = state["claim_data"]

        logger.info(f"ðŸ¤– Starting agentic multi-source analysis for claim {claim_data.get('claim_id')}")

        # ðŸš€ AGENTIC EXTERNAL DATA ENRICHMENT
        try:
            external_enrichment = await agentic_external_manager.comprehensive_claim_enrichment(claim_data)
            state["external_data_enrichment"] = external_enrichment

            logger.info(f"ðŸ¤– External data enrichment complete: {external_enrichment['processing_metadata']['sources_queried']} sources")

        except Exception as e:
            logger.error(f"External data enrichment failed: {e}")
            state["external_data_enrichment"] = {"error": str(e)}

        # Enhanced LLM analysis with external data context
        analysis_prompt = f"""
        ðŸ¤– AGENTIC AI CLAIM ANALYSIS WITH MULTI-SOURCE INTELLIGENCE

        PRIMARY CLAIM DATA:
        Claim ID: {claim_data.get('claim_id')}
        Amount: ${claim_data.get('claim_amount', 0)}
        Type: {claim_data.get('claim_type', 'unknown')}
        Description: {claim_data.get('description', '')}
        Customer: {claim_data.get('customer_name', 'Unknown')}

        EXTERNAL INTELLIGENCE SUMMARY:
        {self._format_external_data_summary(state.get("external_data_enrichment", {}))}

        AGENTIC ANALYSIS REQUIRED:
        1. Priority level (1-10) with multi-source reasoning
        2. Required specialized agents based on external data
        3. Coordination strategy with risk-based routing
        4. Risk indicators from all sources
        5. Fraud score enhancement from external databases
        6. Human escalation requirements
        """
        
        messages = [SystemMessage(content="You are an expert agentic AI insurance claim coordinator with access to multi-source intelligence.")]
        messages.append(HumanMessage(content=analysis_prompt))
        
        try:
            response = await self.llm.ainvoke(messages)
            analysis_result = self._parse_llm_analysis(response.content)

            # Enhance analysis with external data insights
            if "external_data_enrichment" in state and "agentic_analysis" in state["external_data_enrichment"]:
                external_analysis = state["external_data_enrichment"]["agentic_analysis"]
                analysis_result["external_fraud_score"] = external_analysis.get("agentic_risk_assessment", {}).get("composite_risk_score", 0.3)
                analysis_result["external_recommendation"] = external_analysis.get("agentic_recommendations", {}).get("primary_recommendation", "STANDARD_PROCESSING")
                analysis_result["external_sources"] = state["external_data_enrichment"]["processing_metadata"]["sources_queried"]
        except Exception as e:
            logger.error(f"LLM analysis failed: {e}")
            # No fallback analysis - require authentic autonomous reasoning
            raise RuntimeError("Autonomous reasoning failed - manual review required")
        
        state["priority_level"] = analysis_result["priority"]
        state["coordination_strategy"] = analysis_result["strategy"]
        state["reasoning_chain"].append(f"Claim analyzed - Priority: {analysis_result['priority']}")
        
        return state

    def _parse_llm_analysis(self, llm_response: str) -> Dict[str, Any]:
        """Parse LLM response for claim analysis"""
        # Simple parsing logic (enhance with structured output in production)
        priority = 5  # default
        strategy = "parallel"
        
        if "high priority" in llm_response.lower() or "urgent" in llm_response.lower():
            priority = 8
        elif "low priority" in llm_response.lower():
            priority = 3
        
        if "sequential" in llm_response.lower():
            strategy = "sequential"
        
        return {"priority": priority, "strategy": strategy}

    def _fallback_analysis(self, claim_data: Dict[str, Any]) -> Dict[str, Any]:
        """Fallback analysis if LLM fails"""
        amount = claim_data.get("claim_amount", 0)
        priority = 8 if amount > 50000 else 5 if amount > 10000 else 3
        return {"priority": priority, "strategy": "parallel"}

    async def _route_agents_node(self, state: CoordinatorState) -> CoordinatorState:
        """Route claim to appropriate agents"""
        claim_data = state["claim_data"]
        
        # Route to fraud agent
        fraud_routing = route_to_fraud_agent.invoke({"claim_data": claim_data})
        
        # Route to policy agent  
        policy_routing = route_to_policy_agent.invoke({"claim_data": claim_data})
        
        # Determine agent assignments
        state["agent_assignments"] = {
            "fraud_agent": [fraud_routing["priority"]],
            "policy_agent": [policy_routing["priority"]]
        }
        
        state["reasoning_chain"].append("Agents routed: fraud_agent, policy_agent")
        
        return state

    async def _execute_parallel_node(self, state: CoordinatorState) -> CoordinatorState:
        """Execute parallel agent processing"""
        claim_data = state["claim_data"]
        
        # Simulate parallel agent execution (replace with actual HTTP calls)
        fraud_result = await self._call_fraud_agent(claim_data)
        policy_result = await self._call_policy_agent(claim_data)
        
        state["agent_results"] = {
            "fraud_agent": fraud_result,
            "policy_agent": policy_result
        }
        
        state["reasoning_chain"].append("Parallel agent execution completed")
        
        return state

    async def _evaluate_collaboration_node(self, state: CoordinatorState) -> CoordinatorState:
        """Evaluate if agents need to collaborate"""
        
        collaboration_analysis = determine_collaboration_strategy.invoke({"agent_results": state["agent_results"]})
        
        state["collaboration_needed"] = collaboration_analysis["collaboration_needed"]
        state["coordination_strategy"] = collaboration_analysis["strategy"]
        
        state["reasoning_chain"].append(f"Collaboration evaluation: {collaboration_analysis['reasoning']}")
        
        return state

    async def _coordinate_investigation_node(self, state: CoordinatorState) -> CoordinatorState:
        """Coordinate investigation agent if needed"""
        claim_data = state["claim_data"]
        
        # Enhanced claim data with previous agent results
        enhanced_data = {
            **claim_data,
            "fraud_analysis": state["agent_results"]["fraud_agent"],
            "policy_analysis": state["agent_results"]["policy_agent"]
        }
        
        investigation_result = await self._call_investigation_agent(enhanced_data)
        state["agent_results"]["investigation_agent"] = investigation_result
        
        state["reasoning_chain"].append("Investigation agent coordination completed")
        
        return state

    async def _aggregate_results_node(self, state: CoordinatorState) -> CoordinatorState:
        """Aggregate all agent results into AI recommendation"""

        # Create AI recommendation instead of final decision
        ai_recommendation = create_ai_recommendation.invoke({"all_results": state["agent_results"]})

        state["ai_recommendation"] = ai_recommendation
        state["reasoning_chain"].append(f"AI analysis complete - Recommendation ready for human review")

        return state

    async def _human_routing_node(self, state: CoordinatorState) -> CoordinatorState:
        """Route AI recommendation to appropriate human for decision"""

        # Regulatory compliance validation
        compliance_check = self._validate_regulatory_compliance(
            state["claim_data"],
            state["ai_recommendation"],
            state["regulatory_requirements"]
        )

        state["human_routing_decision"] = {
            "routing_status": "routed_to_human",
            "ai_recommendation": state["ai_recommendation"],
            "regulatory_compliance": compliance_check,
            "human_decision_required": True,
            "autonomous_decision_prohibited": "Industry standards require human oversight"
        }

        state["reasoning_chain"].append("Claim routed to human for decision per industry standards")

        return state

    def _validate_regulatory_compliance(
        self,
        claim_data: Dict[str, Any],
        ai_recommendation: Dict[str, Any],
        regulatory_requirements: List[str]
    ) -> Dict[str, Any]:
        """Validate regulatory compliance requirements"""

        compliance_status = "compliant"
        issues = []

        # Check time-sensitive requirements
        if "first_notice_24_hours" in regulatory_requirements:
            # Would check actual timestamps in production
            pass

        # Check fraud reporting requirements
        if ai_recommendation.get("fraud_score", 0) > 0.7:
            if "fraud_reporting_required" not in regulatory_requirements:
                issues.append("High fraud score requires regulatory reporting")
                compliance_status = "non_compliant"

        return {
            "status": compliance_status,
            "issues": issues,
            "requirements_checked": regulatory_requirements,
            "validation_timestamp": datetime.utcnow().isoformat()
        }

    def _needs_collaboration(self, state: CoordinatorState) -> str:
        """Determine if collaboration is needed"""
        return "collaborate" if state["collaboration_needed"] else "aggregate"

    async def _call_fraud_agent(self, claim_data: Dict[str, Any]) -> Dict[str, Any]:
        """Call fraud detection agent"""
        # Simulate fraud agent call (replace with actual HTTP request)
        amount = claim_data.get("claim_amount", 0)
        fraud_score = min(1.0, amount / 100000)  # Simple simulation
        
        return {
            "fraud_score": fraud_score,
            "risk_level": "high" if fraud_score > 0.7 else "medium" if fraud_score > 0.4 else "low",
            "patterns_detected": ["high_amount"] if amount > 50000 else [],
            "recommendation": "investigate" if fraud_score > 0.5 else "approve"
        }

    async def _call_policy_agent(self, claim_data: Dict[str, Any]) -> Dict[str, Any]:
        """Call policy validation agent"""
        # Simulate policy agent call
        return {
            "policy_valid": True,
            "coverage_amount": claim_data.get("claim_amount", 0) * 1.2,
            "issues_found": [],
            "recommendation": "approve"
        }

    async def _call_investigation_agent(self, enhanced_data: Dict[str, Any]) -> Dict[str, Any]:
        """Call investigation agent with enhanced data"""
        # Simulate investigation agent call
        fraud_score = enhanced_data.get("fraud_analysis", {}).get("fraud_score", 0)
        
        return {
            "investigation_depth": "thorough" if fraud_score > 0.6 else "standard",
            "evidence_quality": 0.8,
            "recommendation": "investigate" if fraud_score > 0.7 else "approve",
            "additional_findings": []
        }

    async def coordinate_claim_processing(self, claim_data: Dict[str, Any]) -> Dict[str, Any]:
        """Industry-standard claims coordination with AI assistance and human oversight"""

        start_time = datetime.utcnow()
        claim_id = claim_data.get("claim_id")

        # Regulatory compliance check
        regulatory_triggers = self._check_regulatory_requirements(claim_data)

        # Initialize state for human-supervised processing
        initial_state = CoordinatorState(
            messages=[HumanMessage(content=f"Process claim: {claim_id} with human oversight")],
            claim_data=claim_data,
            agent_assignments={},
            agent_results={},
            coordination_strategy="parallel",
            priority_level=5,
            collaboration_needed=False,
            ai_recommendation=None,
            human_routing_decision=None,
            reasoning_chain=["Starting AI-assisted analysis for human review"],
            regulatory_requirements=regulatory_triggers,
            active_workflows=["human_supervised_claims_processing"]
        )
        
        # Execute AI analysis workflow
        try:
            config = {"configurable": {"thread_id": claim_id or "default"}}
            final_state = await self.app.ainvoke(initial_state, config=config)

            processing_time = (datetime.utcnow() - start_time).total_seconds()

            # Get AI recommendation (not decision)
            ai_recommendation = final_state["ai_recommendation"]

            # Route to appropriate human for decision
            human_routing = await self.human_workflow_manager.route_ai_decision_to_human(
                claim_data=claim_data,
                ai_recommendation=ai_recommendation,
                decision_type="claim_processing_decision"
            )

            # Update metrics
            self.ai_assistance_metrics["recommendations_provided"] += 1

            result = {
                "coordinator_id": self.coordinator_id,
                "claim_id": claim_id,
                "workflow_type": "human_supervised_claims_processing",
                "coordination_strategy": final_state["coordination_strategy"],
                "agent_results": final_state["agent_results"],
                "ai_recommendation": ai_recommendation,
                "human_routing": human_routing,
                "reasoning_chain": final_state["reasoning_chain"],
                "regulatory_requirements": final_state["regulatory_requirements"],
                "processing_time_seconds": processing_time,
                "compliance_status": "industry_standard",
                "human_decision_required": True,
                "ai_assistance_provided": True,
                "timestamp": datetime.utcnow().isoformat()
            }

            return result

        except Exception as e:
            logger.error(f"Error in claim coordination workflow: {str(e)}")
            raise

    def _format_external_data_summary(self, external_data: Dict[str, Any]) -> str:
        """Format external data for LLM analysis"""
        if not external_data or "agentic_analysis" not in external_data:
            return "No external data available"

        analysis = external_data.get("agentic_analysis", {})
        metadata = external_data.get("processing_metadata", {})

        summary = f"""
        ðŸ” EXTERNAL DATA SOURCES: {metadata.get('sources_queried', 0)} consulted
        ðŸ’° COST: {metadata.get('cost', '$0.00')} (Demo Mode)
        âš¡ PROCESSING TIME: {metadata.get('processing_time_ms', 0)}ms

        ðŸš¨ RISK ASSESSMENT:
        - Composite Risk Score: {analysis.get('agentic_risk_assessment', {}).get('composite_risk_score', 0):.2f}
        - Risk Level: {analysis.get('agentic_risk_assessment', {}).get('risk_level', 'UNKNOWN')}
        - Total Risk Indicators: {analysis.get('agentic_risk_assessment', {}).get('total_risk_indicators', 0)}

        ðŸŽ¯ AGENTIC RECOMMENDATIONS:
        - Primary Action: {analysis.get('agentic_recommendations', {}).get('primary_recommendation', 'STANDARD_PROCESSING')}
        - Routing Decision: {analysis.get('agentic_recommendations', {}).get('routing_decision', 'Claims Adjuster')}

        ðŸ“Š CORRELATION INSIGHTS:
        - Cross-source Patterns: {len(analysis.get('correlation_insights', {}).get('cross_source_patterns', []))}
        - Data Consistency: {analysis.get('correlation_insights', {}).get('data_consistency', 'medium')}
        """

        # Add specific risk indicators
        all_indicators = analysis.get('all_risk_indicators', [])
        if all_indicators:
            summary += f"\nðŸ” RISK INDICATORS: {', '.join(all_indicators[:3])}"
            if len(all_indicators) > 3:
                summary += f" (+{len(all_indicators)-3} more)"

        return summary

# FastAPI Application with WebSocket support
app = FastAPI(
    title="Human-Supervised Claims Coordinator",
    description="Industry-standard claims processing with AI assistance and human oversight",
    version="3.0.0"
)

# Global coordinator instance
coordinator: Optional[LangGraphClaimsCoordinator] = None

# WebSocket connections for real-time updates
websocket_connections: List[WebSocket] = []

async def broadcast_processing_update(claim_id: str, update: Dict[str, Any]):
    """Broadcast processing update to WebSocket connections"""
    if websocket_connections:
        message = json.dumps({
            "type": "claim_processing_update",
            "claim_id": claim_id,
            "update": update,
            "timestamp": datetime.utcnow().isoformat()
        })
        
        dead_connections = []
        for ws in websocket_connections:
            try:
                await ws.send_text(message)
            except:
                dead_connections.append(ws)
        
        for dead_ws in dead_connections:
            websocket_connections.remove(dead_ws)

@app.on_event("startup")
async def startup_event():
    global coordinator
    coordinator = LangGraphClaimsCoordinator()
    await db_manager.connect()
    logger.info("Human-Supervised Claims Coordinator and database started with industry compliance")

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    await db_manager.disconnect()
    logger.info("Database connection closed")

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "service": "human-supervised-claims-coordinator",
        "framework": "LangGraph + LangChain + Human Workflow Management",
        "compliance_level": "industry_standard",
        "capabilities": {
            "ai_assistance": [
                "fraud_analysis",
                "policy_validation",
                "investigation_coordination",
                "risk_assessment"
            ],
            "human_oversight": [
                "licensed_professional_routing",
                "regulatory_compliance_enforcement",
                "authority_level_validation",
                "audit_trail_management"
            ],
            "regulatory": [
                "reserved_authority_limits",
                "state_compliance_checking",
                "bad_faith_prevention",
                "time_sensitive_requirements"
            ]
        },
        "metrics": {
            "ai_recommendations_provided": coordinator.ai_assistance_metrics["recommendations_provided"] if coordinator else 0,
            "human_tasks_routed": len(coordinator.human_workflow_manager.active_tasks) if coordinator else 0,
            "regulatory_compliance": "enforced"
        },
        "timestamp": datetime.utcnow().isoformat()
    }

@app.post("/coordinate")
async def coordinate_claim(claim_data: Dict[str, Any]):
    """Coordinate multi-agent claim processing with real-time updates"""
    if not coordinator:
        raise HTTPException(status_code=503, detail="Coordinator not initialized")

    # Generate claim ID if not provided
    import uuid
    claim_id = claim_data.get("claim_id", f"CLM-{datetime.utcnow().strftime('%Y%m%d')}-{str(uuid.uuid4())[:8].upper()}")
    claim_data["claim_id"] = claim_id

    # Save claim to database
    try:
        await db_manager.create_claim(claim_data)
        logger.info(f"Claim {claim_id} saved to database")
    except Exception as e:
        logger.error(f"Failed to save claim {claim_id} to database: {e}")
        # Continue processing even if database save fails

    # Broadcast start of processing
    await broadcast_processing_update(claim_id, {
        "status": "processing_started",
        "priority": "normal"
    })

    try:
        result = await coordinator.coordinate_claim_processing(claim_data)

        # Update claim in database with results including external data
        try:
            update_data = {
                "ai_recommendation": result.get("ai_recommendation"),
                "external_data_analysis": result.get("external_data_enrichment"),
                "fraud_analysis": result.get("fraud_analysis"),
                "policy_analysis": result.get("policy_analysis"),
                "status": "processed",
                "current_stage": "human_review",
                "agentic_analysis_complete": True
            }
            await db_manager.update_claim(claim_id, update_data)
            logger.info(f"ðŸ¤– Claim {claim_id} updated with agentic analysis and external data")
        except Exception as e:
            logger.error(f"Failed to update claim {claim_id} in database: {e}")

        # Broadcast completion
        await broadcast_processing_update(claim_id, {
            "status": "processing_completed",
            "result": result
        })

        return result
    except Exception as e:
        logger.error(f"Error processing claim {claim_id}: {e}")

        # Update claim status in database
        try:
            await db_manager.update_claim(claim_id, {
                "status": "error",
                "error_message": str(e)
            })
        except Exception as db_e:
            logger.error(f"Failed to update error status for claim {claim_id}: {db_e}")

        await broadcast_processing_update(claim_id, {"status": "processing_error", "error": str(e)})
        raise HTTPException(status_code=500, detail=str(e))

@app.websocket("/ws/real-time")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket endpoint for real-time claim processing updates"""
    await websocket.accept()
    websocket_connections.append(websocket)
    logger.info(f"WebSocket connected. Total connections: {len(websocket_connections)}")
    
    try:
        while True:
            # Keep connection alive
            await websocket.receive_text()
    except WebSocketDisconnect:
        if websocket in websocket_connections:
            websocket_connections.remove(websocket)
        logger.info(f"WebSocket disconnected. Total connections: {len(websocket_connections)}")

@app.get("/statistics")
async def get_statistics():
    """Get processing statistics"""
    return {
        "active_websocket_connections": len(websocket_connections),
        "coordinator_status": "active" if coordinator else "inactive",
        "timestamp": datetime.utcnow().isoformat()
    }

@app.get("/workflow")
async def get_workflow_info():
    return {
        "workflow_type": "100% Agentic Multi-Agent Coordination",
        "nodes": [
            "analyze_claim",
            "route_agents", 
            "execute_parallel",
            "evaluate_collaboration",
            "coordinate_investigation",
            "aggregate_results",
            "autonomous_decision"
        ],
        "agentic_capabilities": [
            "ðŸ§  self_learning_from_outcomes",
            "ðŸŽ¯ dynamic_goal_modification", 
            "ðŸ”„ workflow_self_evolution",
            "ðŸŽ¨ creative_problem_solving",
            "ðŸ“Š autonomous_performance_assessment"
        ],
        "autonomy_level": "100%"
    }

# ==================== 100% AGENTIC API ENDPOINTS ====================

@app.post("/learn-outcome")
async def learn_from_outcome_endpoint(
    claim_id: str, 
    predicted_decision: str, 
    actual_outcome: str
):
    """API to teach the system from real outcomes - ACTIVE LEARNING"""
    if not coordinator:
        raise HTTPException(status_code=503, detail="Coordinator not initialized")
    
    await coordinator.learn_from_outcome(claim_id, predicted_decision, actual_outcome)
    return {
        "message": f"ðŸ§  Learning completed for claim {claim_id}",
        "accuracy_improvement": "autonomous_goal_adaptation_triggered",
        "timestamp": datetime.utcnow().isoformat()
    }

@app.get("/performance-assessment")
async def get_autonomous_performance_assessment():
    """Get autonomous self-assessment of system performance"""
    if not coordinator:
        raise HTTPException(status_code=503, detail="Coordinator not initialized")
    
    assessment = await coordinator.autonomous_performance_assessment()
    return {
        "assessment": assessment,
        "autonomy_level": "100%",
        "self_learning_status": "active",
        "timestamp": datetime.utcnow().isoformat()
    }

@app.post("/creative-solution")
async def generate_creative_solution_endpoint(problem_context: Dict[str, Any]):
    """Generate creative solutions for complex cases - CREATIVE PROBLEM SOLVING"""
    if not coordinator:
        raise HTTPException(status_code=503, detail="Coordinator not initialized")
    
    solution = await coordinator.generate_creative_solution(problem_context)
    return {
        "creative_solution": solution,
        "capability": "ðŸŽ¨ autonomous_creative_problem_solving",
        "timestamp": datetime.utcnow().isoformat()
    }

@app.get("/dynamic-goals")
async def get_current_dynamic_goals():
    """See current self-modified goals and adaptations"""
    if not coordinator:
        raise HTTPException(status_code=503, detail="Coordinator not initialized")
    
    return {
        "current_goals": coordinator.dynamic_goals,
        "workflow_optimizations": coordinator.workflow_optimizations,
        "learning_memory_size": len(coordinator.learning_memory),
        "creative_solutions_generated": len(coordinator.creative_solutions),
        "autonomy_status": "ðŸŽ¯ fully_autonomous_goal_adaptation",
        "timestamp": datetime.utcnow().isoformat()
    }

@app.get("/agentic-capabilities")
async def get_agentic_capabilities():
    """Show all 100% agentic capabilities currently active"""
    if not coordinator:
        raise HTTPException(status_code=503, detail="Coordinator not initialized")

    return {
        "autonomy_level": "100%",
        "active_capabilities": {
            "ðŸ§  self_learning": {
                "description": "Learn from prediction outcomes to improve decisions",
                "active": True,
                "cases_learned_from": len(coordinator.learning_memory)
            },
            "ðŸŽ¯ dynamic_goal_modification": {
                "description": "Automatically adapt objectives based on performance",
                "active": True,
                "current_goal": coordinator.dynamic_goals["primary_goal"]
            },
            "ðŸ”„ workflow_self_evolution": {
                "description": "Optimize processing workflows autonomously",
                "active": True,
                "optimizations_made": len(coordinator.workflow_optimizations)
            },
            "ðŸŽ¨ creative_problem_solving": {
                "description": "Generate novel approaches for complex cases",
                "active": True,
                "creative_solutions": len(coordinator.creative_solutions)
            },
            "ðŸ“Š autonomous_performance_assessment": {
                "description": "Self-evaluate and trigger improvements",
                "active": True,
                "last_assessment": "autonomous"
            }
        },
        "system_status": "fully_autonomous_with_continuous_learning",
        "upgrade_from_previous": "82% â†’ 100% agentic",
        "timestamp": datetime.utcnow().isoformat()
    }

@app.get("/human-tasks/{role}")
async def get_human_tasks_by_role(role: str):
    """Get pending human review tasks for a specific role"""
    if not coordinator:
        raise HTTPException(status_code=503, detail="Coordinator not initialized")

    try:
        from .human_workflow_manager import ClaimsRole
        claims_role = ClaimsRole(role)
        tasks = coordinator.human_workflow_manager.get_pending_tasks_by_role(claims_role)

        return {
            "role": role,
            "tasks": tasks,
            "count": len(tasks),
            "timestamp": datetime.utcnow().isoformat()
        }
    except ValueError:
        raise HTTPException(status_code=400, detail=f"Invalid role: {role}")

@app.post("/human-decision/{task_id}")
async def submit_human_decision_to_task(
    task_id: str,
    decision: Dict[str, Any],
    reviewer_id: str,
    reviewer_license: Optional[str] = None
):
    """Submit human decision for a specific task"""
    if not coordinator:
        raise HTTPException(status_code=503, detail="Coordinator not initialized")

    result = await coordinator.human_workflow_manager.submit_human_decision(
        task_id=task_id,
        human_decision=decision,
        reviewer_id=reviewer_id,
        reviewer_license=reviewer_license
    )

    # Update claim in database with human decision
    if result.get("status") == "HUMAN_DECISION_RECORDED":
        task = coordinator.human_workflow_manager.active_tasks.get(task_id)
        if task:
            try:
                await db_manager.update_claim(task.claim_id, {
                    "human_decision": decision,
                    "status": "human_reviewed",
                    "current_stage": "decision_recorded",
                    "reviewed_by": reviewer_id,
                    "reviewed_at": datetime.utcnow()
                })
            except Exception as e:
                logger.error(f"Failed to update claim with human decision: {e}")

    return result

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
